{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 32-bit",
   "display_name": "Python 3.8.5 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "7987c57931f2365761dc2a5f85e647decf73af6edff0c78918b92bfe7c5512ba"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('English')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"\"\"Artificial Intelligence (AI) is an area of computer science that emphasize the creation of intelligent machines that work and react like humans.\"\"\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = word_tokenize(text)\n",
    "\n",
    "new_sentence = []\n",
    "\n",
    "for word in words:\n",
    "    if word not in stop_words: #Jika word tidak ada di stop word maka masuk ke dalam new_sentence.append(word), jika word tidak ada di stop_words berhenti,  jadi word (1,2,3) stop_word (is), untuk menghilangkan stop word jadi diberi suatu kondisi untuk memberhentikannya\n",
    "            new_sentence.append(word)\n",
    "\n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "file = open('adele.txt', 'r')\n",
    "read_file = file.read()\n",
    "text = nltk.Text(nltk.word_tokenize(read_file))\n",
    "\n",
    "match = text.concordance('baby')\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "response = urllib.request.urlopen('http://pontianak.tribunnews.com/')\n",
    "html = response.read()\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import nltk\n",
    "\n",
    "response = urllib.request.urlopen('http://pontianak.tribunnews.com/')\n",
    "html = response.read()\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "text = soup.get_text(strip=True)\n",
    "tokens = [t for t in text.split()]\n",
    "freq = nltk.FreqDist(tokens)\n",
    "for key, val in freq.items():\n",
    "        print(str(key) + ':' + str(val))\n",
    "freq.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import nltk\n",
    "\n",
    "response = urllib.request.urlopen('http://pontianak.tribunnews.com/')\n",
    "html = response.read()\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "text = soup.get_text(strip=True)\n",
    "tokens = [t for t in text.split()]\n",
    "clean_tokens = tokens[:]\n",
    "\n",
    "\n",
    "for token in tokens: \n",
    "    if token in stopwords.words('english'):\n",
    "        clean_tokens.remove(token)\n",
    "freq = nltk.FreqDist(clean_tokens)\n",
    "for key, val in freq.items():\n",
    "        print(str(key) + ':' + str(val))\n",
    "freq.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "mytext = \"Natural Language Processing, usually shortendes as NLP, is a branch of artifical intelligence that deals with the interaction between computers and humans using the natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. Most NLP techniques rely on machine learning to derive meaning form human languages.\"\n",
    "\n",
    "print(sent_tokenize(mytext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "mytext = \"Natural Language Processing, usually shortendes as NLP, is a branch of artifical intelligence that deals with the interaction between computers and humans using the natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable. Most NLP techniques rely on machine learning to derive meaning form human languages.\"\n",
    "\n",
    "print(word_tokenize(mytext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "syn = wordnet.synsets(\"office\")\n",
    "print(syn[0].definition())\n",
    "print(syn[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the branch of information science that deals with natural language information\nlarge Old World boas\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet \n",
    "\n",
    "syn = wordnet.synsets(\"NLP\")\n",
    "print(syn[0].definition())\n",
    "\n",
    "syn = wordnet.synsets(\"Python\")\n",
    "print(syn[0].definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system', 'calculator', 'reckoner', 'figurer', 'estimator', 'computer']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "\n",
    "for syn in wordnet.synsets('Computer'):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['large', 'big', 'big']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets('Small'):\n",
    "    for l in syn.lemmas():\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "work\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('working'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "increase\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('increases'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "play\nplaying\nplaying\nplaying\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('playing', pos=\"v\"))\n",
    "print(lemmatizer.lemmatize('playing', pos=\"n\"))\n",
    "print(lemmatizer.lemmatize('playing', pos=\"a\"))\n",
    "print(lemmatizer.lemmatize('playing', pos=\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "stone\nspeak\nbedroom\njoke\nlisa\npurpl\n__________________________\nstone\nspeaking\nbedroom\njoke\nlisa\npurple\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(stemmer.stem('stones'))\n",
    "print(stemmer.stem('speaking'))\n",
    "print(stemmer.stem('bedroom'))\n",
    "print(stemmer.stem('jokes'))\n",
    "print(stemmer.stem('lisa'))\n",
    "print(stemmer.stem('purple'))\n",
    "\n",
    "print('__________________________')\n",
    "print(lemmatizer.lemmatize('stones'))\n",
    "print(lemmatizer.lemmatize('speaking'))\n",
    "print(lemmatizer.lemmatize('bedroom'))\n",
    "print(lemmatizer.lemmatize('jokes'))\n",
    "print(lemmatizer.lemmatize('lisa'))\n",
    "print(lemmatizer.lemmatize('purple'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}