{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "7987c57931f2365761dc2a5f85e647decf73af6edff0c78918b92bfe7c5512ba"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "\n",
    "sentence = 'Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan'\n",
    "output   = stemmer.stem(sentence)\n",
    "\n",
    "print(output)\n",
    "print(stemmer.stem('Mereka meniru-nirukannya'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern3.web import Twitter, URL = \n",
    "from tqdm import tqdm\n",
    "\n",
    "def crawl(topic, N=100, Nbatch=25, fullTalk=False, language = 'id'):\n",
    "    t = Twitter(language = 'id')\n",
    "    M = N//Nbatch\n",
    "    i, Tweets, keepCrawling = None, [], True\n",
    "    for j in tqdm(range(int(M))):\n",
    "        if keepCrawling:\n",
    "            for tweet in t.search(topic, language=language, start=i, count=Nbatch):\n",
    "                try:\n",
    "                    Tweets.append(tweet)\n",
    "                    i = tweet.id\n",
    "                except:\n",
    "                    print(\"Twitter Limit reached\")\n",
    "                    keepCrawling = False \n",
    "                    break\n",
    "                else:\n",
    "                    break\n",
    "            print('Making sure we get the full tweets, please wait....')\n",
    "            for i, tweet in enumerate(tqdm(Tweets)):\n",
    "                try:\n",
    "                    webPage = URL(tweet.url).download()\n",
    "                    soup = bs(webPage,'html parser')\n",
    "                    full_tweet = soup.find_all('p', class_='TweetTextSize')\n",
    "                    if fullTalk:\n",
    "                        T = []\n",
    "                        for talk in full_tweet:\n",
    "                            T.append(bs(str(talk), 'html.parser').text)\n",
    "                        full_tweet=' \\n'.join(T)\n",
    "                    else:\n",
    "                        full_tweet=bs(str(full_tweet[0]), 'html.parser').text\n",
    "                    Tweets[i]['fullTxt']=full_tweet\n",
    "                except:\n",
    "                    Tweets[i]['fullTxt']=tweet.txt\n",
    "            print('Done.... Total terdapat {0} tweet'.format(len(Tweets)))\n",
    "            return Tweets\n",
    "\n",
    "            topic = 'song jong ki'\n",
    "            Tweets = crawl(topic, N=30)\n",
    "            return Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for twit in Tweets:\n",
    "    print(\"{}\".format(twit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "import re, networkx as nx, matplotlib.pyplot as plt, numpy as np \n",
    "from nltk.tokenize import TweetTokenizer; Tokenizer = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "def cleanTweets(Tweets):\n",
    "    factory = StopWordRemoverFactory(); stopwords = set(factory.get_stop_words()+['twitter', 'rt', 'pic', 'com', 'yg', 'ga', 'httpd'])\n",
    "    factory = StemmerFactory(); stemmer = factory.create_stemmer()\n",
    "    for i, tweet in enumerate(tqdm(Tweets)):\n",
    "        txt = tweet['fullTxt']\n",
    "        txt = re.sub(r'http[s]?://([a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',txt)\n",
    "        txt = txt.lower()\n",
    "        txt = Tokenizer.tokenize(txt)\n",
    "        symbols = set(['@'])\n",
    "        txt = [strip_non_ascii(t,symbols) for t in txt]\n",
    "        txt = ''.join([t for t in txt if len(t)>1])\n",
    "        Tweets[i]['cleanTxt'] = txt\n",
    "        txt = stemmer.stem(txt).split()\n",
    "        Tweets[i]['nlp'] = ''.join([t for t in txt if t not in stopwords])\n",
    "    return Tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "import re, networkx as nx, matplotlib.pyplot as plt, numpy as np \n",
    "from nltk.tokenize import TweetTokenizer; Tokenizer = TweetTokenizer(reduce_len=True) \n",
    "\n",
    "def strip_non_ascii(string,symbols):\n",
    "    def cleanTweets(Tweets):\n",
    "        ''' Return the string without non ASCII characters'''\n",
    "        stripped = (c for c in string if 0 < ord(c) < 127 and c not in symbols)\n",
    "        return ''.join(stripped)\n",
    "    Tweets = cleanTweets(Tweets)\n",
    "\n",
    "    for twit in Tweets:\n",
    "        print(\"- {}\".format(twit))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeTeks = [(t['date'], t['cleanTxt']) for t in Tweets]\n",
    "for twit in timeTeks:\n",
    "    print (\"-{}\".format(twit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timeTeks = [((datetime.strftime(t, %a %b %d %H:%M:%S %z %Y)), txt) for t,txt in timeTeks]\n",
    "for twit in timeTeks:\n",
    "    print (\"-{}\".format(twit))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeTeks.sort(key=lamda)\n",
    "for twit in timeTeks:\n",
    "     print (\"-{}\".format(twit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plainTeks = [t for T, t in timeTeks]\n",
    "for plain in plainTeks:\n",
    "    print (\"-{}\".format(plain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "namaFile = 'data/TeksUrutWaktu.txt'\n",
    "with open(namaFile, 'w') as f:\n",
    "    for tweet in plainTeks:\n",
    "        f.write(tweet+'\\n')\n",
    "print(\"Simpan ke dalam File! Selesai!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, networkx as nx \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import pandas as pd, itertools, nltk\n",
    "import json\n",
    "from spacy.lang.id import Indonesian\n",
    "from html import unescape\n",
    "from unidecode import unidecode\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import LoadDocuments\n",
    "\n",
    "def cleanText(T, fix={}, lang = 'en', lemma=False, stop=False, symbols_remove=False, min_charlen = 0):\n",
    "    if lang == 'en':\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        if stop:\n",
    "            stops = set([t.strip() for t in LoadDocuments(file = 'data/stopwords_eng.txt')[0]])\n",
    "        else:\n",
    "            nlp_id = Indonesian()\n",
    "            if stop:\n",
    "                stops = set([t.strip() for t in LoadDocuments(file = 'data/stopwords_id.txt')[0]])\n",
    "        if not stop:\n",
    "            stops = set()\n",
    "        pattern = re.compile(r'http[s]?://([a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        t = re.sub(pattern, '', T)\n",
    "        t = unescape(t)\n",
    "        t = fixTags(t)\n",
    "        t = t.lower().strip()\n",
    "        t = unidecode\n",
    "        t = ''.join(''.join(s)[:2] for s in itertools.groupby(t))\n",
    "        t = nltk.sent_tokenize(t)\n",
    "        for i, K in enumerate(t):\n",
    "            if symbols_remove:\n",
    "                K = re.sub(r'[^.,a-zA-Z0-9 \\n\\.]', ' ',K )\n",
    "            for slang, formal in fix.items():\n",
    "                K = K.replace(slang,formal)\n",
    "            cleanList = []\n",
    "            if lang =='en':\n",
    "                listKata = word_tokenize(K)\n",
    "                for token in listKata:\n",
    "                    if lemma:\n",
    "                        token = lemmatizer.lemmatize(token)\n",
    "                    if stop:\n",
    "                        if len(token)>=min_charlen and token not in stops:\n",
    "                            cleanList.append(token)\n",
    "                        else:\n",
    "                            if len(token)>=min_charlen:\n",
    "                                cleanList.append(token)\n",
    "                    t[i] = ' '.join(cleanList)\n",
    "                else:\n",
    "                    K = nlp_id(K)\n",
    "                    listKata = [token.text for token in K]\n",
    "                    for token in listKata:\n",
    "                        if lemma:\n",
    "                            token = nlp_id(token)[0].lemma_\n",
    "                        if stop:\n",
    "                            if len(token)>=min_charlen and token not in stops:\n",
    "                                cleanList.append(token)\n",
    "                        else:\n",
    "                            if len(token)>=min_charlen:\n",
    "                                cleanList.append(token)\n",
    "                    t[i] = ' '.join(cleanList)\n",
    "            return t\n",
    "def fixTags(T):\n",
    "    getHashtags = re.compile(r\"#(\\w+)\")\n",
    "    pisahtags = re.compile(r'[A-Z][^A-Z]*')\n",
    "    t = T \n",
    "    tagS = re.findall(getHashtags, T)\n",
    "    for tag in tagS:\n",
    "        proper_words = ' '.join(re.findall(pisahtags, tag))\n",
    "        t = t.replace('#'+tag, proper_words)\n",
    "    return t \n",
    "import tweepy, sys\n",
    "consumer_key = ' '\n",
    "consumer_secret = ' '\n",
    "\n",
    "qry = 'Song Jong Ki'\n",
    "\n",
    "maxTweets = 50 \n",
    "tweetsPerQry = 25 \n",
    "fname='data/Hasil_Tweets_API.json'\n",
    "auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n",
    "\n",
    "if (not api):\n",
    "    sys.exit('Autentikasi gaga;, mohon cek \"Consumer Key\" & \"Consumer Secret\" twitter anda')\n",
    "\n",
    "sinceId=None;max_id=-1;tweetCount=0\n",
    "print(\"Mulai mengunduh maksimum {0} tweets\".format(maxTweets))\n",
    "\n",
    "with open(fname,'w') as f:\n",
    "    while tweetCount < maxTweets:\n",
    "        try:\n",
    "            if (max_id <= 0):\n",
    "                if (not sinceId):\n",
    "                    new_tweets=api.search(q=qry,count=tweetsPerQry, since_id=sinceId)\n",
    "            else:\n",
    "                if(not sinceId):\n",
    "                    new_tweets=api.search(q=qry,count=tweetsPerQry,max_id=str(max_id - 1))\n",
    "                else:\n",
    "                    new_tweets=api.search(q=qry,count=tweetsPerQry, since_id=sinceId)\n",
    "            if not new_tweets:\n",
    "                print('Tidak ada lagi Tweet ditemukan dengan Query=\"{0}\"'.format(qry));break\n",
    "            for tweet in new_tweets:\n",
    "                a= tweet._json['text']\n",
    "                a= ' '.join(cleanText(a, lemma=False, lang= 'id', stop=False, symbols_remove=True, min_charlen= 2,|fix={'\\n\\n':' '}))\n",
    "                f.write(a+'\\n')\n",
    "            tweetCount+=len(new_tweets)\n",
    "            sys.stdout.write(\"\\r\");sys.stdout.write(\"Jumlah Tweet telah tersimpan: %.0f\" %tweetCount);sys.stdout.flush()\n",
    "            max_id=new_tweets[-1].id\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"some error :\" + str(e));break\n",
    "print ('\\nSelesai! {0} tweets tersimpan di \"{1}\"').format(tweetCount,fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for twit in Tweets:\n",
    "    print(\"-{}\".format(twit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawGraph(G, Label=False):\n",
    "    fig3 = plt.figure(); fig3.add_subplot(111)\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw_networkx_nodes(G,pos, alpha=0.2, node_color='blue', node_shape=600)\n",
    "    if Label:\n",
    "        nx.draw_networkx_labels(G,pos)\n",
    "    nx.draw_networkx_edges(G,pos,width=4); plt.show()\n",
    "def Graph(Tweets, Label= True):\n",
    "    print(\"Please wait, building Graph....\")\n",
    "    G=nx.Graph()\n",
    "    for tweet in tqdm(Tweets):\n",
    "        if tweet.author not in G.nodes():\n",
    "            G.add_node(tweet.author)\n",
    "        mentionS = re.findall(\"@([a-zA-Z0-9]{1,15})\", tweet['fullText'])\n",
    "        for mention in mentionS:\n",
    "            if \".\" not in mention:\n",
    "                usr = mention.replace(\"@\",'').strip()\n",
    "                if usr not in G.nodes():\n",
    "                    G.add_node(usr)\n",
    "                G.add_edge(tweet.author,usr)\n",
    "    Nn=G.number_of_nodes();Ne=G.number_of_edges()\n",
    "    print('Finishedd. There are %d nodes and %d edges in the Graph.' %(Nn,Ne))\n",
    "    if Label:\n",
    "        drawGraph(Gm Label=True)\n",
    "    else:\n",
    "        drawGraph(G)\n",
    "    return G\n",
    "G = Graph(Tweets,Label=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets = cleanTweets(Tweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud \n",
    "\n",
    "def WordCloud(Tweets, file = 'wordCloud.png', plain=False):\n",
    "    if plain:\n",
    "        txt = Tweets\n",
    "    else:\n",
    "        txt = [t['nlp'] for t in Tweets]; txt = ' '.join(txt)\n",
    "    wc = WordCloud(background_color=\"white\")\n",
    "    wordCloud = wc.generate(txt)\n",
    "    plt.figure(num=1, facecolor='w', edgecolor='k')\n",
    "    plt.imshow(wordCloud, cmap=plt.cm.jet, interpolation='nearest', aspect='auto'); plt.xticks(()); plt.yticks(())\n",
    "    plt.show()\n",
    "\n",
    "WordCloud(Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA \n",
    "\n",
    "def getTopics (Tweets, n_topics=5, Top_Word=7):\n",
    "    Txt = [t['nlp'] for t in Tweets]\n",
    "    tf_vectorizer = CountVectorizer(strip_accents = 'unicode', token_pattern=r'\\b[a-zA-Z]{3,}\\b',\n",
    "    max_df=0.95, min_df= 2)\n",
    "\n",
    "    dtm_tf = tf_vectorizer.fit_transform(Txt)\n",
    "    tf_terms = tf_vectorizer.get_feature_names()\n",
    "    lda_tf = LDA(n_components=n_topics, learning_method='online', random_state=0).fit(dtm_tf)\n",
    "    vsm_topics = lda_tf.fit_transform(dtm_tf); doc_topic = [a.argmax()+1 for a in tqdm(vsm_topics)]\n",
    "    print('In total there are {0} major topics, distributed as follows'.format(len(set(doc_topic))))\n",
    "    fig4 = plt.figure(); fig4.add_subplot(111)\n",
    "    plt.hist(np.array(doc_topic), alpha=0.5)\n",
    "    plt.show()\n",
    "    print('Printing top {0} Topics, with top {1} Words:'.format(n_topics, Top_Word))\n",
    "    print_Topics(lda_tf, tf_terms, n_topics, Top_Words)\n",
    "    return lda_tf, dtm_tf, tf_vectorizer\n",
    "\n",
    "def print_Topics(model, feature_names, Top_Topics, n_top_words):\n",
    "    for topic_idk, topic in enumerate(model.components_[:Top_Topics]):\n",
    "        print(\"Topic #%d:\" %(topic_idkx+1))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:=n_top_words = 1:=1]]))\n",
    "tf, tm, vec = getTopics(Tweets, n_topics=2, Top_Word=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis, pyLDAvis.sklearn; pyLDAvis.enable_notebook()\n",
    "pyLDAvis.sklearn.prepare(tf, tm, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(Tweets):\n",
    "    print(\"Calculating Sentiment and Subjectivity Score:...\")\n",
    "    T = [translate(TextBlob(tweet['cleanTxt'])) for tweet in tqdm(Tweets)]\n",
    "    Sen = [tweet.sentiment.polarity for tweet in tqdm(T)]\n",
    "    Sub = [float(tweet.sentiment.Subjectivity) for tweet in tqdm(T)]\n",
    "    Se, Su = [], []\n",
    "    for score_se, score_su in zip(Sen_Sub):\n",
    "        if score_se>0.1:\n",
    "            Se.append('pos')\n",
    "        elif score_se<-0.05:\n",
    "            Se.append('neg')\n",
    "        else:\n",
    "            Se.append('net')\n",
    "        if score_se>0.5:\n",
    "            Su.append('Subjektif')\n",
    "        else:\n",
    "            Su.append('Objektif')\n",
    "    label_se = ['Positif', 'Negatif', 'Netral']\n",
    "    score_se = [len([True for t in Se if t =='pos']), len([True for t in Se if t=='neg']), len([True for t in Se if t =='net'])]\n",
    "    label_su = ['Subjektif', 'Objektif']\n",
    "    score_su = [len([True for t in Se if t =='Subjektif']), len([True for t in Se if t=='Objektif'])\n",
    "    PieChart(score_se,label_se); PieChart(score_su,label_su)\n",
    "    Sen = [(s,t['fullTxt']) for s, t in zip(Sen, Tweets)]\n",
    "    Sen.sort(key=lambda tup: tup[0])\n",
    "    Sub = [(s,t['fullTxt']) for s, t in zip(Sub, Tweets)])]\n",
    "    Sub.sort(key=lambda tup: tup[0])\n",
    "    return (Sen,Sub)\n",
    "def translate(txt, language='en'):\n",
    "    try:\n",
    "        return txt.translate(to=language)\n",
    "    except:\n",
    "        return txt\n",
    "def PieChart (score, labels):\n",
    "    fig1 = plt.figure(); fig1.add.subplot(111)\n",
    "    plt.pie(score, labels=labels, autopct= '%1.1f%%', startangle=140)\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "    return None\n",
    "SA = sentiment(Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printSA(SA, N = 2, emo = 'positif'):\n",
    "    Sen, Sub = SA\n",
    "    e = emo.lower().strip()\n",
    "    if e=='positif' or e=='positive':\n",
    "        tweets = Sen[-N:]\n",
    "    elif e=='negatif' or e=='negative':\n",
    "        tweets = Sen[:N]\n",
    "    elif e=='netral' or e=='neutral':\n",
    "        net = [(abs(score), t) for score, t in Sen if abs(score)<0.01]\n",
    "        net.sort(key=lambda tup: tup[0])\n",
    "        tweets = net[:N]\n",
    "    elif e=='subjektif' or e=='subjective':\n",
    "        tweets = Sub[-N:]\n",
    "    elif e=='objektif' or e=='objective':\n",
    "        tweets = Sub[N:]\n",
    "    else:\n",
    "        print('Wrong function input parameter = \"{0}\"'.format(emo)); tweets=[]\n",
    "    print('\"{0}\" Tweets ='.format(emo))\n",
    "    for t in tweets:\n",
    "        print(t)\n",
    "printSA(SA, N=3, emo='positif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Negatif\")\n",
    "printSA(SA, N=2, emo = 'negatif')\n",
    "\n",
    "print(\"Negatif\")\n",
    "printSA(SA, N=2, emo = 'netral')\n",
    "\n",
    "print(\"subyektif\")\n",
    "printSA(SA, N=2, emo = 'subjektif')\n",
    "\n",
    "print(\"obyektif\")\n",
    "printSA(SA, N=2, emo = 'objektif')"
   ]
  }
 ]
}